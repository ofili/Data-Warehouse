# Project: Data Warehouse
---
## Project Overview
A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

In this project, we will create an ETL pipeline to build a data warehouses hosted on Redshift.

## Dataset
---

The dataset is a JSON file in S3, containing metadata on the songs that users have listened to. 

### Song Dataset

The file is a JSON array of objects, with each object representing a song.
It's a subset of real data from <a href="https://millionsongdataset.com" target="_blank">Million Song Dataset</a>. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.

### Log Dataset
---

This consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset are partitioned by year and month.

## Schema for Data Warehouse
---

### Fact Table
**Table: songplays**
The table that contains the following columns: 
```
{songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent}
```	

### Dimension Tables
**Table: users**
The table that contains the following columns: 
```
{user_id, first_name, last_name, gender, level}
```

**Table: songs**
The table contains the following columns:
```
{song_id, title, artist_id, year, duration}
```

**Table: artists**
The table contains the following columns:
```
{artist_id, name, location, latitude, longitude}
```

**Table: time**
The table contains the following columns:
```	
{start_time, hour, day, week, month, year, weekday}
```

## How to use the Data Warehouse
---

### Files

* create_tables.py is the python script that drops all tables and create all tables (including staging tables)

* sql_queries.py is the python file containing all SQL queries. It is called by create_tables.py and etl.py

* etl.py script loads data into staging tables, then inserts data into fact and dimension tables from staging tables.

* main.py script is the main script that calls all function in the create_tables.py and etl.py scripts

* dwh.cfg contains configurations infomation for Redshift database.

* redshift_setup.py sets up the redshift cluster and creates an IAM role for redshift to access other AWS services.

* redshift_teardown.py removes the redshift cluster and the associated IAM role.

### How to Run

#### Set up the redshift cluster and IAM role.
    $ python redshift_setup.py

#### Create tables
    $ python create_tables.py

#### Load data
    $ python etl.py

Reference: [AWS Redshift Doc](https://aws.amazon.com/redshift/getting-started/?p=rs&bttn=hero&exp=b)
